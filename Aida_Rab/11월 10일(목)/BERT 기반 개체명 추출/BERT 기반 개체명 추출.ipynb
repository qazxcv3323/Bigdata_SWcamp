{"cells":[{"cell_type":"markdown","metadata":{"id":"p4R0e9kzdmYx"},"source":["# **1. 필수 라이브러리 설치**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F2iheyjdmY0","outputId":"a6d97872-14d6-43b8-c061-b940397cb874"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.21.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.63.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.5)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.8.17)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["# **2. 라이브러리 불러오기**"],"metadata":{"id":"8fJ2NdrTqmCg"}},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"id":"862iv37FdmY2"},"outputs":[],"source":["import joblib\n","import torch\n","import torch.nn as nn\n","import transformers\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn import preprocessing\n","from sklearn import model_selection\n","\n","from tqdm import tqdm\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup"]},{"cell_type":"markdown","metadata":{"id":"9gWLUBF_dmY3"},"source":["# **3. 학습을 위한 환경 설정**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"id":"ya1iPki-dmY3"},"outputs":[],"source":["class config:\n","    MAX_LEN = 128\n","    TRAIN_BATCH_SIZE = 32\n","    VALID_BATCH_SIZE = 8\n","    EPOCHS = 3\n","    BASE_MODEL_PATH = \"./bert-base-uncased/\"\n","    MODEL_PATH = \"model.bin\"\n","    TRAINING_FILE = \"./data/entity extraction/ner_dataset.csv\"\n","    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n","        BASE_MODEL_PATH,\n","        do_lower_case=True\n","    )"]},{"cell_type":"markdown","metadata":{"id":"ymSnA27fdmY3"},"source":["# **4. 데이터셋 불러오기 및 관리 클래스 정의**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IcFa0ldmdmY4"},"outputs":[],"source":["class EntityDataset:\n","    def __init__(self, texts, pos, tags):\n","        self.texts = texts\n","        self.pos = pos\n","        self.tags = tags\n","    \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, item):\n","        text = self.texts[item]\n","        pos = self.pos[item]\n","        tags = self.tags[item]\n","\n","        ids = []\n","        target_pos = []\n","        target_tag =[]\n","\n","        for i, s in enumerate(text):\n","            inputs = config.TOKENIZER.encode(\n","                s,\n","                add_special_tokens=False\n","            )\n","            # abhishek: ab ##hi ##sh ##ek\n","            input_len = len(inputs)\n","            ids.extend(inputs)\n","            target_pos.extend([pos[i]] * input_len)\n","            target_tag.extend([tags[i]] * input_len)\n","\n","        ids = ids[:config.MAX_LEN - 2]\n","        target_pos = target_pos[:config.MAX_LEN - 2]\n","        target_tag = target_tag[:config.MAX_LEN - 2]\n","\n","        ids = [101] + ids + [102]\n","        target_pos = [0] + target_pos + [0]\n","        target_tag = [0] + target_tag + [0]\n","\n","        mask = [1] * len(ids)\n","        token_type_ids = [0] * len(ids)\n","\n","        padding_len = config.MAX_LEN - len(ids)\n","\n","        ids = ids + ([0] * padding_len)\n","        mask = mask + ([0] * padding_len)\n","        token_type_ids = token_type_ids + ([0] * padding_len)\n","        target_pos = target_pos + ([0] * padding_len)\n","        target_tag = target_tag + ([0] * padding_len)\n","\n","        return {\n","            \"ids\": torch.tensor(ids, dtype=torch.long),\n","            \"mask\": torch.tensor(mask, dtype=torch.long),\n","            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n","            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n","        }"]},{"cell_type":"markdown","metadata":{"id":"SRYl8MxFdmY4"},"source":["# **5. 학습 및 검증용 함수 정의**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"YeKkxbPndmY5"},"outputs":[],"source":["def train_fn(data_loader, model, optimizer, device, scheduler):\n","    model.train()\n","    final_loss = 0\n","    for data in tqdm(data_loader, total=len(data_loader)):\n","        for k, v in data.items():\n","            data[k] = v.to(device)\n","        optimizer.zero_grad()\n","        _, _, loss = model(**data)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        final_loss += loss.item()\n","    return final_loss / len(data_loader)\n","\n","\n","def eval_fn(data_loader, model, device):\n","    model.eval()\n","    final_loss = 0\n","    for data in tqdm(data_loader, total=len(data_loader)):\n","        for k, v in data.items():\n","            data[k] = v.to(device)\n","        _, _, loss = model(**data)\n","        final_loss += loss.item()\n","    return final_loss / len(data_loader)"]},{"cell_type":"markdown","metadata":{"id":"nyPOkRbrdmY5"},"source":["# **6. 손실함수와 모델 정의**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"jXgHCGBsdmY5"},"outputs":[],"source":["def loss_fn(output, target, mask, num_labels):\n","    lfn = nn.CrossEntropyLoss()\n","    active_loss = mask.view(-1) == 1\n","    active_logits = output.view(-1, num_labels)\n","    active_labels = torch.where(\n","        active_loss,\n","        target.view(-1),\n","        torch.tensor(lfn.ignore_index).type_as(target)\n","    )\n","    loss = lfn(active_logits, active_labels)\n","    return loss\n","\n","\n","class EntityModel(nn.Module):\n","    def __init__(self, num_tag, num_pos):\n","        super(EntityModel, self).__init__()\n","        self.num_tag = num_tag\n","        self.num_pos = num_pos\n","        self.bert = transformers.BertModel.from_pretrained(\n","            config.BASE_MODEL_PATH\n","        )\n","        self.bert_drop_1 = nn.Dropout(p=0.3)\n","        self.bert_drop_2 = nn.Dropout(p=0.3)\n","        self.out_tag = nn.Linear(768, self.num_tag)\n","        self.out_pos = nn.Linear(768, self.num_pos)\n","    \n","    def forward(\n","        self, \n","        ids, \n","        mask, \n","        token_type_ids, \n","        target_pos, \n","        target_tag\n","    ):\n","        o1, _ = self.bert(\n","            ids, \n","            attention_mask=mask, \n","            token_type_ids=token_type_ids,\n","            return_dict=False\n","        )\n","\n","        bo_tag = self.bert_drop_1(o1)\n","        bo_pos = self.bert_drop_2(o1)\n","\n","        tag = self.out_tag(bo_tag)\n","        pos = self.out_pos(bo_pos)\n","\n","        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n","        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n","\n","        loss = (loss_tag + loss_pos) / 2\n","\n","        return tag, pos, loss"]},{"cell_type":"markdown","metadata":{"id":"UqDPeJrBdmY6"},"source":["# **7. 데이터 전처리**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3Sldk5QsdmY6"},"outputs":[],"source":["def process_data(data_path):\n","    df = pd.read_csv(data_path, encoding=\"latin-1\")\n","    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n","\n","    enc_pos = preprocessing.LabelEncoder()\n","    enc_tag = preprocessing.LabelEncoder()\n","\n","    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n","    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n","\n","    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n","    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n","    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n","    return sentences, pos, tag, enc_pos, enc_tag"]},{"cell_type":"markdown","metadata":{"id":"Dbw-PO1EdmY6"},"source":["# **8. 모델 학습**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hE7SG1bndmY7","outputId":"e2e1cb54-af89-4db9-a41e-f42b87b0d804"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ./bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","100%|█████████████████████████████████████████████████████████████████████████████████████| 1349/1349 [03:27<00:00,  6.50it/s]\n","100%|███████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:11<00:00, 53.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss = 0.229874825686371 Valid Loss = 0.10626410617803533\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████████████████████████████████████████████████████████████████████████████████| 1349/1349 [03:27<00:00,  6.51it/s]\n","100%|███████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:11<00:00, 51.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss = 0.09573738425804386 Valid Loss = 0.09578821383416652\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████████████████████████████████████████████████████████████████████████████████| 1349/1349 [03:29<00:00,  6.43it/s]\n","100%|███████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:11<00:00, 53.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss = 0.07817878235846736 Valid Loss = 0.0921607621666044\n"]}],"source":["sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n","\n","meta_data = {\n","    \"enc_pos\": enc_pos,\n","    \"enc_tag\": enc_tag\n","}\n","\n","joblib.dump(meta_data, \"meta.bin\")\n","\n","num_pos = len(list(enc_pos.classes_))\n","num_tag = len(list(enc_tag.classes_))\n","\n","(\n","    train_sentences,\n","    test_sentences,\n","    train_pos,\n","    test_pos,\n","    train_tag,\n","    test_tag\n",") = model_selection.train_test_split(\n","    sentences, \n","    pos, \n","    tag, \n","    random_state=42, \n","    test_size=0.1\n",")\n","\n","train_dataset = EntityDataset(\n","    texts=train_sentences, pos=train_pos, tags=train_tag\n",")\n","\n","train_data_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",")\n","\n","valid_dataset = EntityDataset(\n","    texts=test_sentences, pos=test_pos, tags=test_tag\n",")\n","\n","valid_data_loader = torch.utils.data.DataLoader(\n","    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",")\n","\n","device = torch.device(\"cuda\")\n","model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n","model.to(device)\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","optimizer_parameters = [\n","    {\n","        \"params\": [\n","            p for n, p in param_optimizer if not any(\n","                nd in n for nd in no_decay\n","            )\n","        ],\n","        \"weight_decay\": 0.001,\n","    },\n","    {\n","        \"params\": [\n","            p for n, p in param_optimizer if any(\n","                nd in n for nd in no_decay\n","            )\n","        ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","\n","num_train_steps = int(\n","    len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",")\n","optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, \n","    num_warmup_steps=0, \n","    num_training_steps=num_train_steps\n",")\n","\n","best_loss = np.inf\n","for epoch in range(config.EPOCHS):\n","    train_loss = train_fn(\n","        train_data_loader, \n","        model, \n","        optimizer, \n","        device, \n","        scheduler\n","    )\n","    test_loss = eval_fn(\n","        valid_data_loader,\n","        model,\n","        device\n","    )\n","    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n","    if test_loss < best_loss:\n","        torch.save(model.state_dict(), config.MODEL_PATH)\n","        best_loss = test_loss"]},{"cell_type":"markdown","metadata":{"id":"BbYNVhAjdmY7"},"source":["# **9. 개체명 추출**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyIU1FCedmY7","outputId":"8fe0911d-8b49-4aee-ce8e-1d316598e68c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['abhishek', 'is', 'going', 'to', 'india']\n","[101, 11113, 24158, 5369, 2243, 2003, 2183, 2000, 2634, 102]\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ./bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["['B-art' 'B-per' 'B-per' 'B-per' 'B-per' 'O' 'O' 'O' 'B-geo' 'B-art']\n","['$' 'NNP' 'NNP' 'NNP' 'NNP' 'VBZ' 'VBG' 'TO' 'NNP' '$']\n"]}],"source":["meta_data = joblib.load(\"meta.bin\")\n","enc_pos = meta_data[\"enc_pos\"]\n","enc_tag = meta_data[\"enc_tag\"]\n","\n","num_pos = len(list(enc_pos.classes_))\n","num_tag = len(list(enc_tag.classes_))\n","\n","sentence = \"\"\"\n","abhishek is going to india\n","\"\"\"\n","tokenized_sentence = config.TOKENIZER.encode(sentence)\n","\n","sentence = sentence.split()\n","print(sentence)\n","print(tokenized_sentence)\n","\n","test_dataset = EntityDataset(\n","    texts=[sentence], \n","    pos=[[0] * len(sentence)], \n","    tags=[[0] * len(sentence)]\n",")\n","\n","device = torch.device(\"cuda\")\n","model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n","model.load_state_dict(torch.load(config.MODEL_PATH))\n","model.to(device)\n","\n","with torch.no_grad():\n","    data = test_dataset[0]\n","    for k, v in data.items():\n","        data[k] = v.to(device).unsqueeze(0)\n","    tag, pos, _ = model(**data)\n","\n","    print(\n","        enc_tag.inverse_transform(\n","            tag.argmax(2).cpu().numpy().reshape(-1)\n","        )[:len(tokenized_sentence)]\n","    )\n","    print(\n","        enc_pos.inverse_transform(\n","            pos.argmax(2).cpu().numpy().reshape(-1)\n","        )[:len(tokenized_sentence)]\n","    )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}